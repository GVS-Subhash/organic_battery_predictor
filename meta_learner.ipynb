{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to perform meta learning from machine learning. Here, we take the 5 best models from the 5-folds, load their predictions on the validation set, use them as inputs (size (5,1)) and predict the original outputs. We test on the total training set that is used for multi-task ML methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if \"CUDA_VISIBLE_DEVICES\" not in os.environ:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(\n",
    "            gpu, True\n",
    "        )  # Limiting the memory growth\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "import random\n",
    "\n",
    "\n",
    "def set_seeds(seed=SEED):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def set_global_determinism(seed=SEED):\n",
    "    set_seeds(seed=seed)\n",
    "\n",
    "    os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "    os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
    "\n",
    "    # tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "    # tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "\n",
    "# Call the above function with seed value\n",
    "set_global_determinism(seed=SEED)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import optuna\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_json('/home/subhash/test/batterie_informatik/ne/Organic_battery_dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "class MyMinMaxScaler(MinMaxScaler):\n",
    "    def inverse_transform(self, X):\n",
    "        X -= self.min_\n",
    "        X /= self.scale_\n",
    "        return X\n",
    "\n",
    "def log10(x):\n",
    "    return np.log10(x + 1)\n",
    "\n",
    "\n",
    "def log10_inv(x):\n",
    "    return (10 ** x) - 1\n",
    "\n",
    "def get_property_scalers(props):\n",
    "    log10_list = [\"sc\"]\n",
    "    # log first then minmax\n",
    "    scalers = {}\n",
    "    for propertys in props:\n",
    "        mm1 = MyMinMaxScaler()\n",
    "        scalers[propertys] = Pipeline([(\"minmax\", mm1)])\n",
    "    return scalers\n",
    "\n",
    "def get_datasets(df_init):\n",
    "    df = df_init.copy()\n",
    "\n",
    "    df[\"scaled_value\"] = df[\"value\"]\n",
    "\n",
    "    props = df['Property'].unique().tolist()\n",
    "\n",
    "    # Create final dataset for ensemble\n",
    "    ps_final = get_property_scalers(props)\n",
    "\n",
    "    for prop in props:\n",
    "        # train\n",
    "        cond = df[df['Property'] == prop].index\n",
    "        val = df.loc[cond, [\"scaled_value\"]]\n",
    "        df.loc[cond, [\"scaled_value\"]] = ps_final[prop].fit_transform(val)\n",
    "\n",
    "    training_df,val_df=train_test_split(df,test_size=0.2,random_state=0,stratify=df['scala'])\n",
    " \n",
    "    k_fold_datasets = []\n",
    "    skf = StratifiedKFold(n_splits=5,shuffle=False)\n",
    "    for train_index, val_index in skf.split(training_df, training_df['scala']):\n",
    "\n",
    "        k_train = training_df.iloc[train_index].copy()\n",
    "        k_val = training_df.iloc[val_index].copy()\n",
    "\n",
    "        k_fold_datasets.append({\"train\": k_train, \"val\": k_val, \"scaler\":ps_final})\n",
    "\n",
    "    return training_df,k_fold_datasets,val_df,ps_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, total, val, scaler = get_datasets(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna \n",
    "from optuna import trial\n",
    "from pathlib import Path\n",
    "path=Path('/home/subhash/test/batterie_informatik/ne/Multi_task_models')\n",
    "tr_inputs=tf.convert_to_tensor(list(val.cfp))\n",
    "tr_outputs=tf.convert_to_tensor(list(val.scaled_value))\n",
    "te_inputs=tf.convert_to_tensor(list(train.cfp))\n",
    "te_outputs=tf.convert_to_tensor(list(train.scaled_value))\n",
    "tr_i,te_i=[],[]\n",
    "for i in range(5):\n",
    "    mod=tf.keras.models.load_model(f'{path}/{i}.keras')\n",
    "    tr_i.append(mod.predict(tr_inputs))\n",
    "    te_i.append(mod.predict(te_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.reshape(tr_i,newshape=(5,len(tr_i[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=np.reshape(te_i,newshape=(5,len(te_i[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train=a.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_test=b.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train with val and val with train\n",
    "#Take all the models\n",
    "#do predictions\n",
    "#store them in dataframe\n",
    "#Make them as inputs and then train a model in optuna.\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.metrics import R2Score\n",
    "import itertools\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "import numpy as np \n",
    "\n",
    "seed = 123\n",
    "batch_size = 200\n",
    "\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "def create_model(trial):\n",
    "    \n",
    "    n_layers = trial.suggest_int(\"n_layers\", 2, 6)\n",
    "    model = tf.keras.Sequential()\n",
    "    # model.add(tf.keras.layers.Dense(64))\n",
    "    for num in range(n_layers):\n",
    "        num_hidden = trial.suggest_int(\"n_units_l{}\".format(num), 64, 512, step=64)\n",
    "        model.add(tf.keras.layers.Dense(\n",
    "                num_hidden,\n",
    "            )) # Adding dense and dropout alternatively\n",
    "        \n",
    "        activation_name = trial.suggest_categorical(\n",
    "            f\"actvtn_l{num}\", ['relu','LeakyReLU','selu','gelu', 'prelu']\n",
    "        )\n",
    "        if activation_name == \"prelu\":\n",
    "            activation_layer=tf.keras.layers.PReLU()      \n",
    "        elif activation_name == \"LeakyReLU\":\n",
    "            activation_layer=tf.keras.layers.LeakyReLU()\n",
    "        else:\n",
    "            activation_layer=tf.keras.layers.Activation(activation_name)\n",
    "\n",
    "        model.add(activation_layer)\n",
    "\n",
    "        n_drop = trial.suggest_float(\"n_drop_l{}\".format(num), 0, 0.7)\n",
    "        model.add(tf.keras.layers.Dropout(n_drop))\n",
    "    \n",
    "    #model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    train_dataset=tf.data.Dataset.from_tensor_slices((meta_train,tr_outputs))\n",
    "    test_dataset=tf.data.Dataset.from_tensor_slices((meta_test,te_outputs))\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 5e-4, 5e-3, log=True)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    # dataset=dataset\n",
    "    metric = R2Score()\n",
    "    model = create_model(trial)\n",
    "    #rmse=mean_squared_error(squared=False)\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\", metrics=[metric])\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            factor=0.85, monitor=\"val_loss\", verbose=True\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(patience=40, monitor=\"val_loss\", verbose=True)]\n",
    "       \n",
    "    train_dataset = (\n",
    "        train_dataset.cache().batch(50).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "            )  \n",
    "    test_dataset = (\n",
    "            test_dataset.cache().batch(50).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "            )\n",
    "\n",
    "    fit = model.fit(\n",
    "        train_dataset,epochs=500, validation_data=(test_dataset),callbacks=callbacks,batch_size=50)\n",
    "    from pathlib import Path\n",
    "    pt = Path(f'meta_learner/{trial.number}.keras')\n",
    "    pt.parent.mkdir(parents=True, exist_ok=True)\n",
    "    model.save(pt)\n",
    "    return fit.history['val_loss'][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import optuna\n",
    "_dir = locals().get(\"_dir\", None)\n",
    "if not _dir:\n",
    "    _dir = 'meta_learner'\n",
    "\n",
    "study = optuna.create_study(study_name='meta_learner',\n",
    "direction=\"minimize\",storage=f\"sqlite:///{_dir}.db\",load_if_exists=True)\n",
    "\n",
    "study.optimize(objective,n_trials=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
